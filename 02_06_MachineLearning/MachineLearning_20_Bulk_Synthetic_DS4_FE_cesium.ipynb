{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BULK FEATURE EXTRACTION OF THE SYNTHETIC RV CURVES WITH `cesium`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we do the bulk feature extraction with `cesium` for the RV curves in the synthetic Datasets 4 that will be used for validation.\n",
    "\n",
    "**IMPORTANT NOTE:** this code is probably not very efficient (for example, too many dataframe `append` operations, which is costly), but there is no special need at the moment to be more efficient. Maybe the solution is to create a 2D numpy array and then, only at the end, create the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:30:37.817915Z",
     "start_time": "2022-05-04T15:30:37.019314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Module import:\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cesium.data_management import TimeSeries\n",
    "from cesium.featurize import featurize_single_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:30:37.833877Z",
     "start_time": "2022-05-04T15:30:37.821939Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_VAL_DS = 20 #Number of validation datasets\n",
    "\n",
    "\n",
    "#SYNTH_FILE = \"../data/RV_DATASETS/RV_All_GTO_SyntheticDatasets.csv\"\n",
    "RV_DS_FOLDER = \"../data/VAL_DATASETS/\"\n",
    "\n",
    "CS_FEATURES_FOLDER = \"../data/DATASETS_CESIUM/\"\n",
    "OUT_DATASET_GEN_FILE = \"cesium_VAL_DS<number>_Dataset.csv\"\n",
    "\n",
    "# LIST OF STAR METADATA TO ADD (DIFFERENT FOR EACH DATASET):\n",
    "METADATA = {\n",
    "    1: ['ID', 'Pulsating', 'frequency', 'amplitudeRV', 'offsetRV', 'refepochRV', 'phase',\n",
    "        'D1_Ps', 'D1_Tobs'],\n",
    "    2: ['ID', 'Pulsating', 'frequency', 'amplitudeRV', 'offsetRV', 'refepochRV', 'phase',\n",
    "        'D1_Ps', 'D1_Tobs',\n",
    "        'D2_noiseRV_mean', 'D2_noiseRV_median', 'D2_noiseRV_stdev'],\n",
    "    3: ['ID', 'Pulsating', 'frequency', 'amplitudeRV', 'offsetRV', 'refepochRV', 'phase',\n",
    "        'D3_samplingRV_idx', 'D3_PsRV_mean', 'D3_PsRV_median', 'D3_PsRV_stdev', 'D3_NumRV'],\n",
    "    4: ['ID', 'Pulsating', 'frequency', 'amplitudeRV', 'offsetRV', 'refepochRV', 'phase',\n",
    "        'D3_samplingRV_idx', 'D3_PsRV_mean', 'D3_PsRV_median', 'D3_PsRV_stdev', 'D3_NumRV',\n",
    "        'D4_noiseRV_mean', 'D4_noiseRV_median', 'D4_noiseRV_stdev']\n",
    "}\n",
    "\n",
    "# A LIST OF ALL THE FEATURES CESIUM CAN EXTRACT (FOR REFERENCE PURPOSES)\n",
    "ALL_CS_FEATURES = ['all_times_nhist_numpeaks',\n",
    "                   'all_times_nhist_peak1_bin', 'all_times_nhist_peak2_bin', 'all_times_nhist_peak3_bin', 'all_times_nhist_peak4_bin',\n",
    "                   'all_times_nhist_peak_1_to_2', 'all_times_nhist_peak_1_to_3', 'all_times_nhist_peak_1_to_4',\n",
    "                   'all_times_nhist_peak_2_to_3', 'all_times_nhist_peak_2_to_4',\n",
    "                   'all_times_nhist_peak_3_to_4',\n",
    "                   'all_times_nhist_peak_val',\n",
    "                   'avg_double_to_single_step', 'avg_err', 'avgt',\n",
    "                   'cad_probs_1', 'cad_probs_10', 'cad_probs_20', 'cad_probs_30', 'cad_probs_40', 'cad_probs_50',\n",
    "                   'cad_probs_100', 'cad_probs_500', 'cad_probs_1000', 'cad_probs_5000',\n",
    "                   'cad_probs_10000', 'cad_probs_50000', 'cad_probs_100000', 'cad_probs_500000',\n",
    "                   'cad_probs_1000000', 'cad_probs_5000000', 'cad_probs_10000000',\n",
    "                   'cads_avg', 'cads_med', 'cads_std', 'mean',\n",
    "                   'med_double_to_single_step', 'med_err',\n",
    "                   'n_epochs', 'std_double_to_single_step', 'std_err',\n",
    "                   'total_time', 'amplitude',\n",
    "                   'flux_percentile_ratio_mid20', 'flux_percentile_ratio_mid35', 'flux_percentile_ratio_mid50',\n",
    "                   'flux_percentile_ratio_mid65', 'flux_percentile_ratio_mid80',\n",
    "                   'max_slope', 'maximum', 'median', 'median_absolute_deviation', 'minimum',\n",
    "                   'percent_amplitude', 'percent_beyond_1_std', 'percent_close_to_median', 'percent_difference_flux_percentile',\n",
    "                   'period_fast', 'qso_log_chi2_qsonu', 'qso_log_chi2nuNULL_chi2nu', 'skew', 'std',\n",
    "                   'stetson_j', 'stetson_k', 'weighted_average', 'fold2P_slope_10percentile', 'fold2P_slope_90percentile',\n",
    "                   'freq1_amplitude1', 'freq1_amplitude2', 'freq1_amplitude3', 'freq1_amplitude4',\n",
    "                   'freq1_freq', 'freq1_lambda', 'freq1_rel_phase2', 'freq1_rel_phase3', 'freq1_rel_phase4', 'freq1_signif',\n",
    "                   'freq2_amplitude1', 'freq2_amplitude2', 'freq2_amplitude3', 'freq2_amplitude4',\n",
    "                   'freq2_freq', 'freq2_rel_phase2', 'freq2_rel_phase3', 'freq2_rel_phase4',\n",
    "                   'freq3_amplitude1', 'freq3_amplitude2', 'freq3_amplitude3', 'freq3_amplitude4',\n",
    "                   'freq3_freq', 'freq3_rel_phase2', 'freq3_rel_phase3', 'freq3_rel_phase4',\n",
    "                   'freq_amplitude_ratio_21', 'freq_amplitude_ratio_31',\n",
    "                   'freq_frequency_ratio_21', 'freq_frequency_ratio_31',\n",
    "                   'freq_model_max_delta_mags', 'freq_model_min_delta_mags', 'freq_model_phi1_phi2',\n",
    "                   'freq_n_alias', 'freq_signif_ratio_21', 'freq_signif_ratio_31',\n",
    "                   'freq_varrat', 'freq_y_offset', 'linear_trend', 'medperc90_2p_p',\n",
    "                   'p2p_scatter_2praw', 'p2p_scatter_over_mad', 'p2p_scatter_pfold_over_mad', 'p2p_ssqr_diff_over_var',\n",
    "                   'scatter_res_raw']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction with `cesium` for the synthetic RV curves of Datasets 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T16:20:17.724173Z",
     "start_time": "2022-05-04T15:35:50.335001Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset 0\n",
      "Record: 0, started at 04/05/2022, 17:35:50...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:36:24...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 200, started at 04/05/2022, 17:37:04...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 17:37:40...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 1\n",
      "Record: 0, started at 04/05/2022, 17:38:06...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:38:45...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Record: 200, started at 04/05/2022, 17:39:12...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 17:39:47...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Processing validation dataset 2\n",
      "Record: 0, started at 04/05/2022, 17:40:28...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:41:02...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Record: 200, started at 04/05/2022, 17:41:32...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Record: 300, started at 04/05/2022, 17:41:59...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 3\n",
      "Record: 0, started at 04/05/2022, 17:42:41...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:43:07...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 200, started at 04/05/2022, 17:43:39...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 17:44:09...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 4\n",
      "Record: 0, started at 04/05/2022, 17:44:50...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:45:33...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 200, started at 04/05/2022, 17:46:05...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 17:46:39...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Processing validation dataset 5\n",
      "Record: 0, started at 04/05/2022, 17:47:21...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:47:45...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 200, started at 04/05/2022, 17:48:18...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 300, started at 04/05/2022, 17:49:06...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 6\n",
      "Record: 0, started at 04/05/2022, 17:49:35...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:50:18...\n",
      "Previous median lapse time: 0.19 seconds\n",
      "Record: 200, started at 04/05/2022, 17:50:46...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Record: 300, started at 04/05/2022, 17:51:12...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Processing validation dataset 7\n",
      "Record: 0, started at 04/05/2022, 17:51:54...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:52:36...\n",
      "Previous median lapse time: 0.20 seconds\n",
      "Record: 200, started at 04/05/2022, 17:53:08...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Record: 300, started at 04/05/2022, 17:53:34...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 8\n",
      "Record: 0, started at 04/05/2022, 17:54:05...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:54:42...\n",
      "Previous median lapse time: 0.14 seconds\n",
      "Record: 200, started at 04/05/2022, 17:55:17...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 17:55:52...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Processing validation dataset 9\n",
      "Record: 0, started at 04/05/2022, 17:56:32...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:57:04...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 200, started at 04/05/2022, 17:57:38...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Record: 300, started at 04/05/2022, 17:58:04...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 10\n",
      "Record: 0, started at 04/05/2022, 17:58:35...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 17:58:59...\n",
      "Previous median lapse time: 0.14 seconds\n",
      "Record: 200, started at 04/05/2022, 17:59:32...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 300, started at 04/05/2022, 18:00:10...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 11\n",
      "Record: 0, started at 04/05/2022, 18:00:49...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:01:29...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Record: 200, started at 04/05/2022, 18:02:06...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Record: 300, started at 04/05/2022, 18:02:34...\n",
      "Previous median lapse time: 0.17 seconds\n",
      "Processing validation dataset 12\n",
      "Record: 0, started at 04/05/2022, 18:03:08...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:03:40...\n",
      "Previous median lapse time: 0.21 seconds\n",
      "Record: 200, started at 04/05/2022, 18:04:19...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Record: 300, started at 04/05/2022, 18:04:45...\n",
      "Previous median lapse time: 0.18 seconds\n",
      "Processing validation dataset 13\n",
      "Record: 0, started at 04/05/2022, 18:05:14...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:05:40...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 200, started at 04/05/2022, 18:06:21...\n",
      "Previous median lapse time: 0.14 seconds\n",
      "Record: 300, started at 04/05/2022, 18:06:55...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Processing validation dataset 14\n",
      "Record: 0, started at 04/05/2022, 18:07:27...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:07:54...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 200, started at 04/05/2022, 18:08:20...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 18:08:47...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Processing validation dataset 15\n",
      "Record: 0, started at 04/05/2022, 18:09:24...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:09:57...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 200, started at 04/05/2022, 18:10:30...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 18:11:04...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Processing validation dataset 16\n",
      "Record: 0, started at 04/05/2022, 18:11:34...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:11:58...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 200, started at 04/05/2022, 18:12:22...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 300, started at 04/05/2022, 18:13:09...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Processing validation dataset 17\n",
      "Record: 0, started at 04/05/2022, 18:13:42...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:14:11...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 200, started at 04/05/2022, 18:14:41...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Record: 300, started at 04/05/2022, 18:15:12...\n",
      "Previous median lapse time: 0.15 seconds\n",
      "Processing validation dataset 18\n",
      "Record: 0, started at 04/05/2022, 18:15:50...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:16:22...\n",
      "Previous median lapse time: 0.22 seconds\n",
      "Record: 200, started at 04/05/2022, 18:16:54...\n",
      "Previous median lapse time: 0.19 seconds\n",
      "Record: 300, started at 04/05/2022, 18:17:25...\n",
      "Previous median lapse time: 0.19 seconds\n",
      "Processing validation dataset 19\n",
      "Record: 0, started at 04/05/2022, 18:18:03...\n",
      "Previous median lapse time: None\n",
      "Record: 100, started at 04/05/2022, 18:18:42...\n",
      "Previous median lapse time: 0.19 seconds\n",
      "Record: 200, started at 04/05/2022, 18:19:07...\n",
      "Previous median lapse time: 0.16 seconds\n",
      "Record: 300, started at 04/05/2022, 18:19:43...\n",
      "Previous median lapse time: 0.18 seconds\n"
     ]
    }
   ],
   "source": [
    "# DISABLE WARNINGS:\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#CS_FEATURES_FOLDER = \"../data/DATASETS_CESIUM/\"\n",
    "#OUT_DATASET_GEN_FILE = \"cesium_DS<number>_Dataset.csv\"\n",
    "\n",
    "#for j in range(0, 2): # TEST\n",
    "for j in range(0, NUM_VAL_DS):\n",
    "    print(\"Processing validation dataset %d\" %j)\n",
    "    ds_subfolder = \"VAL_DS-\" + str(j) + \"/\"\n",
    "    # Load dataset info table:\n",
    "    synth_file = RV_DS_FOLDER + \"VAL_DS-\" + str(j) + \"/VAL_DS-\" + str(j) + \"_SynthDatasets.csv\"\n",
    "    synth = pd.read_csv(synth_file, sep=',', decimal='.')\n",
    "    # Initialize features dataframes and metafeatures:\n",
    "    df = {\n",
    "        1: None,\n",
    "        2: None,\n",
    "        3: None,\n",
    "        4: None\n",
    "    }\n",
    "    i0=0\n",
    "\n",
    "    # Batch processing:\n",
    "    lapse_list = []\n",
    "    median_lapse = None\n",
    "    # Initialize features dataframes and metafeatures (from disk, or new):\n",
    "    metadata_idx = METADATA\n",
    "\n",
    "    #for i in range(0, 3): # TEST\n",
    "    for i in range(i0, len(synth)):\n",
    "        start_time = time.time()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Record: %d, started at %s...\"\n",
    "                  %(i, time.strftime('%d/%m/%Y, %H:%M:%S', time.localtime(start_time))))\n",
    "            if median_lapse is None:\n",
    "                print(\"Previous median lapse time: %s\" %median_lapse)\n",
    "            else:\n",
    "                print(\"Previous median lapse time: %.2f seconds\" %median_lapse)\n",
    "        for ds in [4]: # Only DS4\n",
    "        #for ds in [1, 2, 3, 4]:\n",
    "            # For each dataset:\n",
    "            # Get metafeatures values:\n",
    "            metadata_values = list(synth.loc[i, metadata_idx[ds]])\n",
    "            try:\n",
    "                # load RV file:\n",
    "                filename = synth.loc[i, 'ds' + str(ds) + '_file']\n",
    "                rv = pd.read_csv(filename, sep=' ', decimal='.',\n",
    "                                 names=['time', 'rv'])\n",
    "                # Create TimeSeries object:\n",
    "                ts = TimeSeries(t=rv['time'], m=rv['rv'])\n",
    "                # Featurize the time series:\n",
    "                cs = featurize_single_ts(ts, features_to_use=ALL_CS_FEATURES)\n",
    "                # Join metadata and features for the dataframe:\n",
    "                indices = metadata_idx[ds] + ['VALID_RECORD'] + list(cs.index.get_level_values('feature'))\n",
    "                values = metadata_values + [True] + list(cs.values)\n",
    "            except Exception as e:\n",
    "                # An exception was found, mark the record as invalid and set the features to 'nan':\n",
    "                print(\"***ERROR: some error happened in record %d, dataset %d, \" \\\n",
    "                      \"marking the record as invalid. Error: %s\"\n",
    "                      %(i, ds, str(e)))\n",
    "                indices = metadata_idx[ds] + ['VALID_RECORD'] + ALL_CS_FEATURES\n",
    "                values = metadata_values + [False] + [np.nan] * 112\n",
    "            if df[ds] is None:\n",
    "                # Initialize DataFrame (with the first item):\n",
    "                df[ds] = pd.DataFrame(data=[values], columns=indices)\n",
    "            else:\n",
    "                # Create a new DataFrame (with the new item):\n",
    "                new_df = pd.DataFrame(data=[values], columns=indices)\n",
    "                # Append the new dataframe to the existing one:\n",
    "                df[ds] = df[ds].append(new_df, ignore_index=True)\n",
    "            # UPDATE THE AVERAGE RECORD PROCESSING TIME:\n",
    "            lapse = time.time() - start_time\n",
    "            lapse_list.append(lapse)\n",
    "            median_lapse = np.nanmedian(lapse_list)\n",
    "            # Save the results:\n",
    "            df[ds].to_csv(CS_FEATURES_FOLDER + OUT_DATASET_GEN_FILE.replace(\"<number>\", str(j) + \"_\" + str(ds)),\n",
    "                          sep=',', decimal='.', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps are to be executed only if the cell execution is user-interrupted\n",
    "\n",
    "For example, if the user decided to interrupt the cell execution because it got stuck in some record, the next cells update the info for that record with an \"invalid record\" mark.\n",
    "\n",
    "Afterwards, the loop (previous cell) can be executed again and it will start from the record following the problematic one."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:25:02.060594Z",
     "start_time": "2022-04-28T10:22:22.066Z"
    }
   },
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:25:02.062588Z",
     "start_time": "2022-04-28T10:22:22.714Z"
    }
   },
   "source": [
    "df[1].tail()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:25:02.063584Z",
     "start_time": "2022-04-28T10:22:23.418Z"
    }
   },
   "source": [
    "synth.loc[i]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:25:02.063584Z",
     "start_time": "2022-04-28T10:22:24.105Z"
    }
   },
   "source": [
    "# Update the wrong record:\n",
    "for ds in [1 , 2, 3, 4]:\n",
    "    indices = metadata_idx[ds] + ['VALID_RECORD'] + ALL_CS_FEATURES\n",
    "    metadata_values = list(synth.loc[i, metadata_idx[ds]])\n",
    "    values = metadata_values + [False] + [np.nan] * 112\n",
    "    new_df = pd.DataFrame(data=[values], columns=indices)\n",
    "    df[ds] = df[ds].append(new_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:25:02.064582Z",
     "start_time": "2022-04-28T10:22:24.706Z"
    }
   },
   "source": [
    "df[1].tail()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:25:02.065595Z",
     "start_time": "2022-04-28T10:22:25.250Z"
    }
   },
   "source": [
    "# Save the results:\n",
    "for ds in [1, 2, 3, 4]\n",
    "    df[ds].to_csv(CS_FEATURES_FOLDER + OUT_DATASET_GEN_FILE.replace(\"<number>\", str(ds)),\n",
    "                  sep=',', decimal='.', index=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:25:02.066576Z",
     "start_time": "2022-04-28T10:22:25.755Z"
    }
   },
   "source": [
    "df[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the records with errors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-28T10:44:32.117447Z",
     "start_time": "2022-04-28T10:44:32.091516Z"
    }
   },
   "source": [
    "df[1][df[1]['VALID_RECORD'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSIONS:**\n",
    "- Completed the `cesium` feature extraction of the DS4 synthetic datasets.\n",
    "- No problems were found in calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de contenidos",
   "title_sidebar": "Contenidos",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
