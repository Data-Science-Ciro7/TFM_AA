{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5657ed60",
   "metadata": {},
   "source": [
    "# CARMENES RV ADDITIONAL SYNTHETIC DATASET 4 CURVES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e7ec8",
   "metadata": {},
   "source": [
    "In this notebook we create twenty additional _Datasets 4_, different from the one we used for training and validate the model.\n",
    "\n",
    "The idea behind this operation is to see how our model performs globally on other synthetic datasets, created under the same initial conditions but different from the one used to optimize and train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b1a5a",
   "metadata": {},
   "source": [
    "## Modules and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569d8c2",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db817188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:36.878116Z",
     "start_time": "2022-05-04T15:05:36.026177Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from distfit import distfit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\", {'figure.figsize':(15,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad459b",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044384a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:36.893346Z",
     "start_time": "2022-05-04T15:05:36.880116Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION:\n",
    "N = 400 # Number of records (i.e. RV curves) to create / add.\n",
    "RANDOM_STATE = 11 # For reproducibility\n",
    "NUM_DS = 20 # Number of datasets to create.\n",
    "\n",
    "NON_PULSATION_FRACTION = 0.29 # Fraction of non-pulsating stars\n",
    "\n",
    "DIST_SUMMARY_FILE = \"../02_04_SyntheticDataset/DIST_FILES/Parameter_distributions_All_GTO.csv\"\n",
    "DIST_FILES_FOLDER = \"../02_04_SyntheticDataset/DIST_FILES/\"\n",
    "RV_PATTERNS_FILE = \"../02_04_SyntheticDataset/DIST_FILES/RV_All_GTO_sampling_patterns.csv\"\n",
    "\n",
    "DS_FOLDER = \"../data/VAL_DATASETS/\"\n",
    "DATASETS_SUMMARY_FILE = \"../data/VAL_DATASETS/RV_All_GTO_SyntheticDatasets.csv\"\n",
    "\n",
    "DS_PREFIX=\"VAL_DS-\" # RECOMMENDED - DO NOT CHANGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9226a8",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb58654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:36.908542Z",
     "start_time": "2022-05-04T15:05:36.895304Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_value(t, f, A, c, tau, delta, noise):\n",
    "    '''Returns the value of a benchmark sample at a time 't', depending on the values of the paremeters passed:\n",
    "    f: frequency; A: amplitude; c: offset; tau:reference epoch; delta:phase'''\n",
    "    value = A * np.cos(2 * np.pi * (f * (t - tau) + delta)) + c + noise\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8f5817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:36.924503Z",
     "start_time": "2022-05-04T15:05:36.909543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141592653589793"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee5eba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:36.939344Z",
     "start_time": "2022-05-04T15:05:36.925501Z"
    }
   },
   "outputs": [],
   "source": [
    "# We vectorize the previous function to be able to pass arrays as parameters,\n",
    "# in order to calculate the time series in one call\n",
    "v_sample_value = np.vectorize(sample_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12b5c6",
   "metadata": {},
   "source": [
    "## Prepare distributions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6620d",
   "metadata": {},
   "source": [
    "### Read configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ed581",
   "metadata": {},
   "source": [
    "Read the file containing the distributions and values used for dataset generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc77e09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:36.955302Z",
     "start_time": "2022-05-04T15:05:36.940342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Distribution</th>\n",
       "      <th>Dist_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENCHMARK</td>\n",
       "      <td>Ps</td>\n",
       "      <td>{'fixed_value': 0.0025}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENCHMARK</td>\n",
       "      <td>Tobs</td>\n",
       "      <td>{'fixed_value': 0.25}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENCHMARK</td>\n",
       "      <td>frequency</td>\n",
       "      <td>{'distr': &lt;scipy.stats._continuous_distns.unif...</td>\n",
       "      <td>BENCHMARK_frequency_All_GTO_dist.pickle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BENCHMARK</td>\n",
       "      <td>phase</td>\n",
       "      <td>{'distr': &lt;scipy.stats._continuous_distns.unif...</td>\n",
       "      <td>BENCHMARK_phase_All_GTO_dist.pickle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BENCHMARK</td>\n",
       "      <td>amplitudeRV</td>\n",
       "      <td>{'distr': &lt;scipy.stats._continuous_distns.unif...</td>\n",
       "      <td>BENCHMARK_amplitudeRV_All_GTO_dist.pickle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Group     Variable                                       Distribution  \\\n",
       "0  BENCHMARK           Ps                            {'fixed_value': 0.0025}   \n",
       "1  BENCHMARK         Tobs                              {'fixed_value': 0.25}   \n",
       "2  BENCHMARK    frequency  {'distr': <scipy.stats._continuous_distns.unif...   \n",
       "3  BENCHMARK        phase  {'distr': <scipy.stats._continuous_distns.unif...   \n",
       "4  BENCHMARK  amplitudeRV  {'distr': <scipy.stats._continuous_distns.unif...   \n",
       "\n",
       "                                   Dist_file  \n",
       "0                                        NaN  \n",
       "1                                        NaN  \n",
       "2    BENCHMARK_frequency_All_GTO_dist.pickle  \n",
       "3        BENCHMARK_phase_All_GTO_dist.pickle  \n",
       "4  BENCHMARK_amplitudeRV_All_GTO_dist.pickle  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_params = pd.read_csv(DIST_SUMMARY_FILE, sep=',', decimal='.')\n",
    "dist_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7957e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:36.971272Z",
     "start_time": "2022-05-04T15:05:36.956305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Group           object\n",
       "Variable        object\n",
       "Distribution    object\n",
       "Dist_file       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_params.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c99e86",
   "metadata": {},
   "source": [
    "### Register the distributions to use for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d15a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:37.003110Z",
     "start_time": "2022-05-04T15:05:36.973253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/BENCHMARK_frequency_All_GTO_dist.pickle]\n",
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/BENCHMARK_phase_All_GTO_dist.pickle]\n",
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/BENCHMARK_amplitudeRV_All_GTO_dist.pickle]\n",
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/BENCHMARK_offsetRV_All_GTO_dist.pickle]\n",
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/BENCHMARK_refepochRV_All_GTO_dist.pickle]\n",
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/RV_noiseRV_All_GTO_dist.pickle]\n",
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/RV_samplingperiodRV_All_GTO_dist.pickle]\n",
      "[pypickle] Pickle file loaded: [../02_04_SyntheticDataset/DIST_FILES/RV_numsamplesRV_All_GTO_dist.pickle]\n"
     ]
    }
   ],
   "source": [
    "# Initialise variable:\n",
    "param_ranges = {}\n",
    "for i in range(0,len(dist_params)):\n",
    "    try:\n",
    "        # If successful, it is a fixed value:\n",
    "        d = json.loads(dist_params.loc[i, 'Distribution'].replace('\\'', '\\\"'))\n",
    "        param_ranges[dist_params.loc[i, 'Variable']] = d\n",
    "    except:\n",
    "        # Must be a distribution, so we load it from file:\n",
    "        try:\n",
    "            d = distfit()\n",
    "            d.load(DIST_FILES_FOLDER + dist_params.loc[i, 'Dist_file'])\n",
    "            param_ranges[dist_params.loc[i, 'Variable']] = d\n",
    "        except Exception as e:\n",
    "            # Some error happened:\n",
    "            print(\"***ERROR! Could not set parameter %s. Error: %s\" \\\n",
    "                  %(dist_params.loc[i, 'Variable'], str(e)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e651bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:37.017913Z",
     "start_time": "2022-05-04T15:05:37.005114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ps': {'fixed_value': 0.0025},\n",
       " 'Tobs': {'fixed_value': 0.25},\n",
       " 'frequency': <distfit.distfit.distfit at 0x1adde59ed90>,\n",
       " 'phase': <distfit.distfit.distfit at 0x1adde540fa0>,\n",
       " 'amplitudeRV': <distfit.distfit.distfit at 0x1adde5b7940>,\n",
       " 'offsetRV': <distfit.distfit.distfit at 0x1adda6a5520>,\n",
       " 'refepochRV': <distfit.distfit.distfit at 0x1adde5ba700>,\n",
       " 'noiseRV': <distfit.distfit.distfit at 0x1adde5baca0>,\n",
       " 'samplingperiodRV': <distfit.distfit.distfit at 0x1adde5d2a30>,\n",
       " 'numsamplesRV': <distfit.distfit.distfit at 0x1adde5e1b50>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show result:\n",
    "param_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937be50",
   "metadata": {},
   "source": [
    "## Create the random datasets and the RV curve files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b53897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:37.033876Z",
     "start_time": "2022-05-04T15:05:37.018910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.2699525 , 21.54508497, 15.54496738, 24.75528258, 17.7300475 ,\n",
       "       18.45491503, 24.45503262, 15.24471742, 22.2699525 , 21.54508497,\n",
       "       15.54496738, 24.75528258, 17.7300475 , 18.45491503, 24.45503262,\n",
       "       15.24471742, 22.2699525 , 21.54508497, 15.54496738, 24.75528258,\n",
       "       17.7300475 , 18.45491503, 24.45503262, 15.24471742])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 1 - Vectorized function: clean time series.\n",
    "v_sample_value(t=[0.00, 0.50, 1.00, 1.50, 2.00, 2.50, 3.00, 3.50, 4.00, 4.50, 5.00, 5.50, 6.00, \\\n",
    "                  6.50, 7.00, 7.50, 8.00, 8.50, 9.00, 9.50, 10.00, 10.50, 11.00, 11.50],\n",
    "               f=0.75, A=5.0, c=20.0, tau=0.5, delta=0.2,\n",
    "               noise=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8e9589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:37.049827Z",
     "start_time": "2022-05-04T15:05:37.034867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.76928434, 20.16524066, 14.96693216, 23.95727816, 19.27376819,\n",
       "       19.64381925, 24.29369588, 15.14258395, 21.1417928 , 22.47903277,\n",
       "       15.11388455, 23.88963744, 19.69837251, 18.52497498, 25.79773337,\n",
       "       15.14231051, 23.87502571, 21.63492746, 13.55160556, 24.5650102 ,\n",
       "       17.79201912, 20.29102676, 23.25331584, 15.02353347])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 2 - Vectorized function: noisy time series.\n",
    "v_sample_value(t=[0.00, 0.50, 1.00, 1.50, 2.00, 2.50, 3.00, 3.50, 4.00, 4.50, 5.00, 5.50, 6.00, \\\n",
    "                  6.50, 7.00, 7.50, 8.00, 8.50, 9.00, 9.50, 10.00, 10.50, 11.00, 11.50],\n",
    "               f=0.75, A=5.0, c=20.0, tau=0.5, delta=0.2,\n",
    "               noise=[-1.500668159, -1.379844309, -0.578035218, -0.798004417, 1.543720686, 1.188904223, \\\n",
    "                      -0.161336742, -0.102133465, -1.1281597, 0.933947801, -0.431082827, -0.865645144, \\\n",
    "                      1.968325011, 0.070059952, 1.342700752, -0.102406912, 1.605073208, 0.089842491, \\\n",
    "                      -1.993361815, -0.190272377, 0.061971617, 1.836111736, -1.201716785, -0.221183952])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a41eb",
   "metadata": {},
   "source": [
    "### Create the samples of time series in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a9ead97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:05:46.417005Z",
     "start_time": "2022-05-04T15:05:46.392070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sampling_delta_RV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 16.990309999790043, 34.978099999949336, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 2.009560000151396, 10.982160000130534, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 11.003340000286698, 29.86382000008598, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 11.957520000170916, 47.87878999998793, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 17.016419999767095, 190.38824999984354, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sampling_delta_RV\n",
       "0  [0.0, 16.990309999790043, 34.978099999949336, ...\n",
       "1  [0.0, 2.009560000151396, 10.982160000130534, 1...\n",
       "2  [0.0, 11.003340000286698, 29.86382000008598, 5...\n",
       "3  [0.0, 11.957520000170916, 47.87878999998793, 5...\n",
       "4  [0.0, 17.016419999767095, 190.38824999984354, ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the sampling patterns from the RV patterns file:\n",
    "rv_patterns = pd.read_csv(RV_PATTERNS_FILE)\n",
    "rv_patterns['sampling_delta_RV'] = rv_patterns['sampling_delta_RV'] \\\n",
    "    .map(lambda x: json.loads(x))\n",
    "rv_patterns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7cb5e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T15:07:53.905028Z",
     "start_time": "2022-05-04T15:06:19.378075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset 0...\n",
      "Creating validation dataset 1...\n",
      "Creating validation dataset 2...\n",
      "Creating validation dataset 3...\n",
      "Creating validation dataset 4...\n",
      "Creating validation dataset 5...\n",
      "Creating validation dataset 6...\n",
      "Creating validation dataset 7...\n",
      "Creating validation dataset 8...\n",
      "Creating validation dataset 9...\n",
      "Creating validation dataset 10...\n",
      "Creating validation dataset 11...\n",
      "Creating validation dataset 12...\n",
      "Creating validation dataset 13...\n",
      "Creating validation dataset 14...\n",
      "Creating validation dataset 15...\n",
      "Creating validation dataset 16...\n",
      "Creating validation dataset 17...\n",
      "Creating validation dataset 18...\n",
      "Creating validation dataset 19...\n"
     ]
    }
   ],
   "source": [
    "#for j in range(0, 2): # TEST\n",
    "for j in range(0, NUM_DS):\n",
    "    print(\"Creating validation dataset %d...\" %j)\n",
    "    # Initialize:\n",
    "    ds_subfolder = DS_FOLDER + DS_PREFIX + str(j) + \"/\"\n",
    "    if os.path.isdir(ds_subfolder):\n",
    "        # Folder exists:\n",
    "        pass\n",
    "    else:\n",
    "        # Create the folder:\n",
    "        os.mkdir(ds_subfolder)\n",
    "    dataset_file = ds_subfolder + DS_PREFIX + str(j) + \"_SynthDatasets.csv\"\n",
    "    datasets = pd.DataFrame(columns=['ID', 'Pulsating', \\\n",
    "                                     'frequency', 'amplitudeRV', 'offsetRV', 'refepochRV', 'phase', \\\n",
    "                                     'D1_Ps', 'D1_Tobs', \\\n",
    "                                     'D2_noiseRV_mean', 'D2_noiseRV_median', 'D2_noiseRV_stdev', \\\n",
    "                                     'D3_samplingRV_idx', 'D3_PsRV_mean', 'D3_PsRV_median', \\\n",
    "                                     'D3_PsRV_stdev', 'D3_NumRV', \\\n",
    "                                     'D4_noiseRV_mean', 'D4_noiseRV_median', 'D4_noiseRV_stdev',\n",
    "                                     'ds1_file', 'ds2_file', 'ds3_file', 'ds4_file'])\n",
    "    \n",
    "\n",
    "    #for i in range(0, 5): # TEST\n",
    "    for i in range(len(datasets), len(datasets) + N):\n",
    "        #if True: # TEST\n",
    "        try:\n",
    "            step = \"Main parameters\"\n",
    "            # Set the record sequential ID and the pulsation characteristic:\n",
    "            record_id = \"RV-\" + str(i)\n",
    "            datasets.loc[i, 'ID'] = record_id\n",
    "            if np.asscalar(np.random.rand(1)) <= NON_PULSATION_FRACTION:\n",
    "                # Non-pulsating star:\n",
    "                datasets.loc[i, 'Pulsating'] = False\n",
    "                # Choose the main parameter values:\n",
    "                # (one value per record, shared by all 4 distributions)\n",
    "                # In this case, only the distributions for the offsetRV and refepochRV are used:\n",
    "                frequency = 0.0\n",
    "                amplitudeRV = 0.0\n",
    "                offsetRV = param_ranges['offsetRV'].generate(n=1, verbose=0)\n",
    "                refepochRV = param_ranges['refepochRV'].generate(n=1, verbose=0)\n",
    "                phase = 0.0\n",
    "                # Populate the dataset table with the basic parameters:\n",
    "                datasets.loc[i, 'frequency'] = frequency\n",
    "                datasets.loc[i, 'amplitudeRV'] = amplitudeRV\n",
    "                datasets.loc[i, 'offsetRV'] = np.asscalar(offsetRV)\n",
    "                datasets.loc[i, 'refepochRV'] = np.asscalar(refepochRV)\n",
    "                datasets.loc[i, 'phase'] = phase\n",
    "            else:\n",
    "                # Pulsating star:\n",
    "                datasets.loc[i, 'Pulsating'] = True\n",
    "                # Choose the main parameter values:\n",
    "                # (one value per record, shared by all 4 distributions)\n",
    "                # All the dsistributions are used to calculate random parameters:\n",
    "                frequency = param_ranges['frequency'].generate(n=1, verbose=0)\n",
    "                amplitudeRV = param_ranges['amplitudeRV'].generate(n=1, verbose=0)\n",
    "                offsetRV = param_ranges['offsetRV'].generate(n=1, verbose=0)\n",
    "                refepochRV = param_ranges['refepochRV'].generate(n=1, verbose=0)\n",
    "                phase = param_ranges['phase'].generate(n=1, verbose=0)\n",
    "                # Populate the dataset table with the basic parameters:\n",
    "                datasets.loc[i, 'frequency'] = np.asscalar(frequency)\n",
    "                datasets.loc[i, 'amplitudeRV'] = np.asscalar(amplitudeRV)\n",
    "                datasets.loc[i, 'offsetRV'] = np.asscalar(offsetRV)\n",
    "                datasets.loc[i, 'refepochRV'] = np.asscalar(refepochRV)\n",
    "                datasets.loc[i, 'phase'] = np.asscalar(phase)\n",
    "\n",
    "            # DS1: Generate the benchmark (noiseless, perfectly sampled) time series:\n",
    "            step = \"DS1 calculation\"        \n",
    "            # Get the timestamps parameters (sampling period and observation time):\n",
    "            Ps = param_ranges['Ps']['fixed_value']\n",
    "            Tobs = param_ranges['Tobs']['fixed_value']\n",
    "            # Generate the timestamps (absolute values):\n",
    "            ds1_time = np.linspace(refepochRV, refepochRV+Tobs, int(Tobs/Ps) + 1, endpoint=True)\n",
    "            # Generate the values:\n",
    "            ds1_value = v_sample_value(t=ds1_time,\n",
    "                                   f=frequency, A=amplitudeRV, c=offsetRV, tau=refepochRV, delta=phase,\n",
    "                                   noise=0.0)\n",
    "            ds1_ts = np.stack([ds1_time, ds1_value], axis=1).reshape(-1,2)\n",
    "            # Create the DS1 filename:\n",
    "            ds1_file = ds_subfolder + \"DS1-\" + record_id + \".dat\"\n",
    "            # Store the DS1 file:\n",
    "            np.savetxt(ds1_file, ds1_ts, delimiter=' ')\n",
    "            # Populate the dataset table with the relevant data for DS1:\n",
    "            datasets.loc[i, 'D1_Ps'] = Ps\n",
    "            datasets.loc[i, 'D1_Tobs'] = Tobs\n",
    "            datasets.loc[i, 'ds1_file'] = ds1_file\n",
    "        \n",
    "            # DS2: Generate the noise and create the noisy time series:\n",
    "            step = \"DS2 calculation\"        \n",
    "            # Generate a noise array equal in length to  DS1:\n",
    "            # (Note that the noise can be randomly positive or negative)\n",
    "            ds2_noise_value = np.random.choice([-1, 1], size=len(ds1_time)) * \\\n",
    "                param_ranges['noiseRV'].generate(n=len(ds1_time), verbose=0)\n",
    "            ds2_noise_value = ds2_noise_value.reshape(-1,1)\n",
    "            # Generate DS2 time series (just the sum of two series):\n",
    "            ds2_value = ds1_value + ds2_noise_value\n",
    "            ds2_ts = np.stack([ds1_time, ds2_value], axis=1).reshape(-1,2)\n",
    "            # Calculate noise stats:\n",
    "            ds2_noise_mean = np.asscalar(np.nanmean(ds2_noise_value.flatten()))\n",
    "            ds2_noise_median = np.asscalar(np.nanmedian(ds2_noise_value.flatten()))\n",
    "            ds2_noise_std = np.asscalar(np.nanstd(ds2_noise_value.flatten()))\n",
    "            # Create the DS2 filename:\n",
    "            ds2_file = ds_subfolder + \"DS2-\" + record_id + \".dat\"\n",
    "            # Store the DS2 file:\n",
    "            np.savetxt(ds2_file, ds2_ts, delimiter=' ')\n",
    "            # Populate the dataset table with the relevant data for DS2:\n",
    "            datasets.loc[i, 'D2_noiseRV_mean'] = ds2_noise_mean\n",
    "            datasets.loc[i, 'D2_noiseRV_median'] = ds2_noise_median\n",
    "            datasets.loc[i, 'D2_noiseRV_stdev'] = ds2_noise_std\n",
    "            datasets.loc[i, 'ds2_file'] = ds2_file\n",
    "        \n",
    "            # DS3: Generate the imperfectly sampled time series:\n",
    "            step = \"DS3 calculation\"        \n",
    "            # Choose a random sampling pattern from the pool and generate the timestamps:\n",
    "            pattern_idx = np.asscalar(np.random.randint(low=0, high=len(rv_patterns), size=1))\n",
    "            rv_t_deltas = rv_patterns.iloc[pattern_idx, 0]\n",
    "            # Generate DS3 time series:\n",
    "            ds3_time = refepochRV + rv_t_deltas\n",
    "            ds3_value = v_sample_value(t=ds3_time,\n",
    "                                   f=frequency, A=amplitudeRV, c=offsetRV, tau=refepochRV, delta=phase,\n",
    "                                   noise=0.0)\n",
    "            ds3_ts = np.stack([ds3_time, ds3_value], axis=1).reshape(-1,2)\n",
    "\n",
    "            # Calculate sampling stats:\n",
    "            ds3_deltas = ds3_time[1:] - ds3_time[:-1]\n",
    "            ds3_PsRV_mean = np.asscalar(np.nanmean(ds3_deltas))\n",
    "            ds3_PsRV_median = np.asscalar(np.nanmedian(ds3_deltas))\n",
    "            ds3_PsRV_stdev = np.asscalar(np.std(ds3_deltas))\n",
    "            ds3_NumRV = len(ds3_time)\n",
    "            # Create the DS3 filename:\n",
    "            ds3_file = ds_subfolder + \"DS3-\" + record_id + \".dat\"\n",
    "            # Store the DS3 file:\n",
    "            np.savetxt(ds3_file, ds3_ts, delimiter=' ')\n",
    "            # Populate the dataset table with the relevant data for DS3:\n",
    "            datasets.loc[i, 'D3_samplingRV_idx'] = pattern_idx\n",
    "            datasets.loc[i, 'D3_PsRV_mean'] = ds3_PsRV_mean\n",
    "            datasets.loc[i, 'D3_PsRV_median'] = ds3_PsRV_median\n",
    "            datasets.loc[i, 'D3_PsRV_stdev'] = ds3_PsRV_stdev\n",
    "            datasets.loc[i, 'D3_NumRV'] = ds3_NumRV\n",
    "            datasets.loc[i, 'ds3_file'] = ds3_file\n",
    "\n",
    "            # DS4: Generate the noisy and imperfectly sampled time series:\n",
    "            step = \"DS4 calculation\"        \n",
    "            # Generate a noise array equal in length to DS3:\n",
    "            ds4_noise_value = np.random.choice([-1, 1], size=len(ds3_time)) * \\\n",
    "                param_ranges['noiseRV'].generate(n=len(ds3_time), verbose=0)\n",
    "            ds4_noise_value = ds4_noise_value.reshape(-1,1)\n",
    "            # Generate DS4 time series (just the sum of two series):\n",
    "            ds4_value = ds3_value + ds4_noise_value.reshape(-1,)\n",
    "            ds4_ts = np.stack([ds3_time, ds4_value], axis=1).reshape(-1,2)\n",
    "            # Calculate noise stats:\n",
    "            ds4_noise_mean = np.asscalar(np.nanmean(ds4_noise_value.flatten()))\n",
    "            ds4_noise_median = np.asscalar(np.nanmedian(ds4_noise_value.flatten()))\n",
    "            ds4_noise_std = np.asscalar(np.nanstd(ds4_noise_value.flatten()))\n",
    "            # Create the DS4 filename:\n",
    "            ds4_file = ds_subfolder + \"DS4-\" + record_id + \".dat\"\n",
    "            # Store the DS2 file:\n",
    "            np.savetxt(ds4_file, ds4_ts, delimiter=' ')\n",
    "            # Populate the dataset table with the relevant data for DS4:\n",
    "            datasets.loc[i, 'D4_noiseRV_mean'] = ds4_noise_mean\n",
    "            datasets.loc[i, 'D4_noiseRV_median'] = ds4_noise_median\n",
    "            datasets.loc[i, 'D4_noiseRV_stdev'] = ds4_noise_std\n",
    "            datasets.loc[i, 'ds4_file'] = ds4_file\n",
    "    \n",
    "            # Store the dataset table so far:\n",
    "            datasets.to_csv(dataset_file, sep=',', decimal='.', index=False)\n",
    "    \n",
    "        #else: # TEST\n",
    "        except Exception as e:\n",
    "            # Report any possible trouble:\n",
    "            print(\"***ERROR: some error happened when generating record %d, at step '%s' Error: %s\" \\\n",
    "                  %(i, step, str(e)))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d3740",
   "metadata": {},
   "source": [
    "### Show the last curves generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05a341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:07:33.090587Z",
     "start_time": "2022-04-26T12:07:33.060631Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd128ef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-23T12:00:02.854745Z",
     "start_time": "2022-04-23T12:00:02.843744Z"
    }
   },
   "source": [
    "#### Basic parameters of the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c2677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:07:33.106150Z",
     "start_time": "2022-04-26T12:07:33.091585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic parameters:\n",
    "print(\"PARAMETERS OF THE CURVE:\")\n",
    "if datasets.loc[i, 'Pulsating'] == True:\n",
    "    print(\"Star type: Pulsating star\")\n",
    "    print(\"Amplitude: %f\" %amplitudeRV[0])\n",
    "    print(\"Frequency: %f\" %frequency[0])\n",
    "    print(\"Reference epoch: %f\" %refepochRV[0])\n",
    "    print(\"Phase: %f\" %phase[0])\n",
    "    print(\"Offset: %f\" %offsetRV[0])\n",
    "    print(\"Benchmark (perfect) sampling period: %f\" %Ps)\n",
    "    print(\"Benchmark (perfect) observation time: %f\" %Tobs)\n",
    "else:\n",
    "    print(\"Star type: Non-pulsating star\")\n",
    "    print(\"Amplitude: %f\" %amplitudeRV)\n",
    "    print(\"Frequency: %f\" %frequency)\n",
    "    print(\"Reference epoch: %f\" %refepochRV[0])\n",
    "    print(\"Phase: %f\" %phase)\n",
    "    print(\"Offset: %f\" %offsetRV[0])\n",
    "    print(\"Benchmark (perfect) sampling period: %f\" %Ps)\n",
    "    print(\"Benchmark (perfect) observation time: %f\" %Tobs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1efd00",
   "metadata": {},
   "source": [
    "#### Simplified, quick plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a71a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:07:33.308489Z",
     "start_time": "2022-04-26T12:07:33.107148Z"
    }
   },
   "outputs": [],
   "source": [
    "# DS1 benchmark curve:\n",
    "plt.plot(ds1_time, ds1_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659e59e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T14:58:12.983222Z",
     "start_time": "2022-04-26T14:58:12.849824Z"
    }
   },
   "outputs": [],
   "source": [
    "# DS1 benchmark curve - Only 1 period:\n",
    "try:\n",
    "    # Pulsating star:\n",
    "    plt.plot(ds1_time, ds1_value)\n",
    "    plt.xlim(ds1_time.min(), ds1_time.min()+1/frequency[0])\n",
    "except:\n",
    "    # Non pulsating star:\n",
    "    plt.plot(ds1_time, ds1_value)\n",
    "    #plt.xlim(ds1_time.min(), ds1_time.min()+1/frequency)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b32a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T14:58:17.106214Z",
     "start_time": "2022-04-26T14:58:16.961568Z"
    }
   },
   "outputs": [],
   "source": [
    "# DS1 and DS2 curves:\n",
    "plt.plot(ds1_time, ds1_value, label='DS1')\n",
    "plt.plot(ds1_time, ds2_value, label='DS2')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267979e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T14:58:19.339488Z",
     "start_time": "2022-04-26T14:58:19.177359Z"
    }
   },
   "outputs": [],
   "source": [
    "# DS3 and DS4 curves:\n",
    "plt.plot(ds3_time, ds3_value, label='DS3')\n",
    "plt.plot(ds3_time, ds4_value, label='DS4')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bcc34",
   "metadata": {},
   "source": [
    "#### More elaborated plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b34f17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T14:59:32.653747Z",
     "start_time": "2022-04-26T14:59:32.049483Z"
    }
   },
   "outputs": [],
   "source": [
    "# DS1 and DS2 curves:\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Example - Synthetic curves generated with CARMENES parameters\\n(Non-pulsating star. Noiseless curve and noisy curve)\", fontsize=16)\n",
    "plt.grid(axis='both', alpha=0.75)\n",
    "plt.xlabel(\"Time [JD]\", fontsize=12)\n",
    "plt.ylabel('RV [$ms^{-1}$]', fontsize=12)\n",
    "sns.lineplot(x=ds1_time.flatten(), y=ds1_value.flatten(), label='DS1 - Noiseless curve')\n",
    "sns.lineplot(x=ds1_time.flatten(), y=ds2_value.flatten(), label='DS2 - Noisy curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf5507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T14:59:35.655773Z",
     "start_time": "2022-04-26T14:59:35.329600Z"
    }
   },
   "outputs": [],
   "source": [
    "# DS3 and DS4 curves:\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Example - Synthetic curves generated with CARMENES parameters\\n\" \\\n",
    "          \"(Non-pulsating star. Irregularly sampled curves: noiseless and noisy)\", fontsize=16)\n",
    "plt.grid(axis='both', alpha=0.75)\n",
    "plt.xlabel(\"Time [JD]\", fontsize=12)\n",
    "plt.ylabel('RV [$ms^{-1}$]', fontsize=12)\n",
    "sns.lineplot(x=ds3_time.flatten(), y=ds3_value.flatten(), label='DS3 - Irregularly sampled, noiseless curve')\n",
    "sns.lineplot(x=ds3_time.flatten(), y=ds4_value.flatten(), label='DS4 - Irregularly sampled, noisy curve')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb5610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
